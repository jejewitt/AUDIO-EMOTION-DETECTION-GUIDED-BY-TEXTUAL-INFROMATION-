{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMBINED MELD MODEL",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywDax4nQFQWQ",
        "outputId": "736fe334-0232-45c6-c533-487f4739f837"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"./drive/MyDrive/Project/\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cmeh0Ff9Bkn"
      },
      "source": [
        "%%capture\n",
        "!pip install np_utils\n",
        "!pip install transformers\n",
        "!pip install pickle5"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEIfK27I9GY4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import pickle\n",
        "import re\n",
        "import pickle5 as pickle\n",
        "\n",
        "from sklearn.model_selection import train_test_split,KFold,StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score,f1_score \n",
        "from sklearn.preprocessing import LabelEncoder,normalize,OneHotEncoder,LabelBinarizer\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import  MaxPool2D,Dense,Input,Convolution2D,Flatten,Activation,Dropout\n",
        "from keras import models,optimizers,losses\n",
        "from keras.utils import np_utils\n",
        "from keras.activations import softmax, relu \n",
        "from keras import backend as K\n",
        "\n",
        "from transformers import RobertaTokenizer, RobertaModel, TFRobertaForSequenceClassification\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgSiOD7d-s5z"
      },
      "source": [
        "%%capture\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JDzVP67-pAU"
      },
      "source": [
        "def convert_mfcc(path, bands, sample,duration):\n",
        "    mf = np.empty(shape=(len(path), bands, 376, 1))\n",
        "    total_len = sample*duration\n",
        "    for i,one_path in enumerate(path):\n",
        "        data,sr = librosa.load(one_path,sr=sample,res_type=\"kaiser_fast\",duration=duration,offset=0.8) # offeset is the start\n",
        "        data_len = len(data)\n",
        "        if data_len > total_len:\n",
        "            offset = np.random.randint((data_len - total_len))\n",
        "            data = data[offset:(total_len+offset)]\n",
        "        elif total_len > data_len:\n",
        "            offset = np.random.randint(( total_len-data_len))\n",
        "            data = np.pad(data, (offset, int(total_len) - len(data) - offset), \"constant\")\n",
        "        else:\n",
        "            data = np.pad(data, (0, int(total_len) - len(data)), \"constant\")\n",
        "        mf[i,] = np.expand_dims(librosa.feature.mfcc(data, sr=sample, n_mfcc=bands), axis=-1)\n",
        "    return mf\n",
        "def get_model(bands):\n",
        "    input = Input(shape=(bands,376,1))\n",
        "    X = Convolution2D(64, (10,30), padding=\"same\")(input)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation(\"relu\")(X)\n",
        "    X = MaxPool2D()(X)\n",
        "    X = Dropout(rate=0.2)(X)\n",
        "    \n",
        "    X = Convolution2D(64, (10,30), padding=\"same\")(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation(\"relu\")(X)\n",
        "    X = MaxPool2D()(X)\n",
        "    X = Dropout(rate=0.2)(X)\n",
        "\n",
        "    \n",
        "    X = Convolution2D(64, (10,30), padding=\"same\")(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation(\"relu\")(X)\n",
        "    X = MaxPool2D()(X)\n",
        "    X = Dropout(rate=0.75)(X)\n",
        "\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(64)(X)\n",
        "    X = Dropout(rate=0.2)(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation(\"relu\")(X)\n",
        "    X = Dropout(rate=0.2)(X)\n",
        "    \n",
        "    out = Dense(7, activation=softmax)(X)\n",
        "    model = models.Model(inputs=input, outputs=out)\n",
        "    model.compile(optimizer=optimizers.Adam(0.001), loss=losses.categorical_crossentropy, metrics=['acc',f1_m])\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLYIzowD_UZq"
      },
      "source": [
        "\n",
        "wav_list = []\n",
        "folders = [\"out\"]\n",
        "empty = []\n",
        "for i in folders:\n",
        "    da= os.listdir(f\"MELD/{i}\")    \n",
        "    for k in da:\n",
        "        one_path = f\"MELD/{i}/\"+ k\n",
        "        if  '.wav' in k: # '.mp3' in k or\n",
        "            if os.path.getsize(one_path)!=0:\n",
        "                wav_list.append(one_path) \n",
        "                # durations.append(eyed3.load(one_path).info.time_secs)\n",
        "            else:\n",
        "                empty.append(one_path) \n",
        "                \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHOPeG8o_W9_"
      },
      "source": [
        "train = pd.read_csv(\"train_sent_emo.csv\")\n",
        "test = pd.read_csv(\"test_sent_emo.csv\")\n",
        "validation = pd.read_csv(\"dev_sent_emo.csv\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUdSV7G5_zt9"
      },
      "source": [
        "label = []\n",
        "index = []\n",
        "for i in wav_list:\n",
        "    # print(i)\n",
        "    if '(1)' not in i:\n",
        "        dia = int(re.split('/|_|\\.|test', i)[-3][3:])\n",
        "        utt = int(re.split('/|_|\\.|test', i)[-2][3:])\n",
        "        e = train[(train[\"Dialogue_ID\"]==dia) & (train[\"Utterance_ID\"]==utt)]['Emotion'].tolist()[0]\n",
        "        index.append(train[(train[\"Dialogue_ID\"]==dia) & (train[\"Utterance_ID\"]==utt)]['Emotion'].index.tolist()[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GusWFRehPqbK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol23VV_SPq4-"
      },
      "source": [
        "LOAD precomputed mfcc bands"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SvQDLdkACSq"
      },
      "source": [
        "mfcc = []\n",
        "with open('mfcc_meld_pc_4.pkl', 'rb') as handle:\n",
        "    mfcc = pickle.load(handle)\n",
        "label = []\n",
        "with open('label_meld_pc_4.pkl', 'rb') as handle:\n",
        "    label = pickle.load(handle)\n",
        "index = []\n",
        "with open('info_3.pkl', 'rb') as handle:\n",
        "    index = pickle.load(handle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da8St52tB-Hl"
      },
      "source": [
        "train = pd.read_csv(\"train_sent_emo.csv\")\n",
        "test = pd.read_csv(\"test_sent_emo.csv\")\n",
        "validation = pd.read_csv(\"dev_sent_emo.csv\")\n",
        "train = train.loc[index, :]\n",
        "s = pd.Series(list(range(len(train))))\n",
        "train=train.set_index(s)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGl2sX6dCAod"
      },
      "source": [
        "sample=48000\n",
        "duration=4\n",
        "bands = 30\n",
        "\n",
        "teee = range(len(mfcc))\n",
        "\n",
        "X_train, X_test, train_dat, test_dat = train_test_split(mfcc\n",
        "                                                    , teee\n",
        "                                                    , test_size=0.2\n",
        "                                                    , shuffle=True\n",
        "                                                    , random_state=42)\n",
        "\n",
        "y_train = [label[i] for i in train_dat]\n",
        "y_test = [label[i] for i in test_dat]\n",
        "\n",
        "rrrr = train.loc[test_dat, :]\n",
        "train  = train.loc[train_dat, :]\n",
        "train = train.append(test, ignore_index=True)\n",
        "train = train.append(validation, ignore_index=True)\n",
        "test = rrrr\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjM-D7wTyvZI"
      },
      "source": [
        "from sklearn.utils import class_weight\n",
        "class_weights = dict(zip(range(len(np.unique(y_train))), class_weight.compute_class_weight('balanced', np.unique(y_train), \n",
        "                y_train))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McDpwsFmJzhi"
      },
      "source": [
        "lb = LabelEncoder()\n",
        "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
        "y_test = np_utils.to_categorical(lb.fit_transform(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y2gfIiXoINA"
      },
      "source": [
        "# from https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PNQAb0ypRr5"
      },
      "source": [
        "# (64, (10,30)\n",
        "def get_model(bands):\n",
        "    input = Input(shape=(bands,376,1))\n",
        "    X = Convolution2D(16, (5,5), padding=\"same\")(input)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation(\"relu\")(X)\n",
        "    X = MaxPool2D()(X)\n",
        "    X = Dropout(rate=0.2)(X)\n",
        "    \n",
        "    X = Convolution2D(16, (5,5), padding=\"same\")(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation(\"relu\")(X)\n",
        "    X = MaxPool2D()(X)\n",
        "    X = Dropout(rate=0.2)(X)\n",
        "    X = Convolution2D(16, (5,5), padding=\"same\")(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation(\"relu\")(X)\n",
        "    X = MaxPool2D()(X)\n",
        "    X = Dropout(rate=0.2)(X)\n",
        "\n",
        "    \n",
        "    X = Convolution2D(16, (5,5), padding=\"same\")(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation(\"relu\")(X)\n",
        "    X = MaxPool2D()(X)\n",
        "    X = Dropout(rate=0.7)(X)\n",
        "\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(64)(X)\n",
        "    X = Dropout(rate=0.2)(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = Activation(\"relu\")(X)\n",
        "    X = Dropout(rate=0.2)(X)\n",
        "    \n",
        "    out = Dense(7, activation=softmax)(X)\n",
        "    model = models.Model(inputs=input, outputs=out)\n",
        "    model.compile(optimizer=optimizers.Adam(0.001), loss=losses.categorical_crossentropy, metrics=['acc',f1_m])\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SsPOvAoJuGz"
      },
      "source": [
        "\n",
        "mean = np.mean(X_train, axis=0)\n",
        "std = np.std(X_train, axis=0)\n",
        "\n",
        "X_train = (X_train - mean)/std\n",
        "X_test = (X_test - mean)/std\n",
        "checkpoint = ModelCheckpoint(filepath='mel.h5', \n",
        "                             monitor='val_f1_m',\n",
        "                             verbose=1, \n",
        "                             save_best_only=True,\n",
        "                             mode='max')\n",
        "# Build CNN model \n",
        "model = get_model(bands)\n",
        "\n",
        "model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
        "                    batch_size=64, verbose = 2, epochs=200 ,callbacks=[checkpoint]) #, class_weight=class_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbsJOsfqqwIc"
      },
      "source": [
        "# %%capture cap --no-stderr\n",
        "# KF = KFold(n_splits=5, random_state=42, shuffle=True) #Stratified\n",
        "# for train_index, test_index in KF.split(mfcc,label):\n",
        "#     checkpoint = ModelCheckpoint(filepath='e.h5', \n",
        "#                              monitor='val_f1_m',\n",
        "#                              verbose=1, \n",
        "#                              save_best_only=True,\n",
        "#                              mode='max')\n",
        "#     X_train, X_test = mfcc[train_index], mfcc[test_index]\n",
        "#     y_train, y_test = np.asarray(label)[train_index], np.asarray(label)[test_index]\n",
        "\n",
        "#     lb = LabelEncoder()\n",
        "#     y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
        "#     y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n",
        "\n",
        "#     mean = np.mean(X_train, axis=0)\n",
        "#     std = np.std(X_train, axis=0)\n",
        "\n",
        "#     X_train = (X_train - mean)/std\n",
        "#     X_test = (X_test - mean)/std\n",
        "\n",
        "#     model = get_model(bands)\n",
        "#     model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
        "#                         batch_size=64, verbose = 2, epochs=200,callbacks=[checkpoint]) \n",
        "    \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEMKrZupo2B6",
        "outputId": "995331f8-32ba-47f3-91c0-7201ee9be5a2"
      },
      "source": [
        "\n",
        "with open('output_meld.txt', 'w') as f:\n",
        "    f.write(cap.stdout)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.utils.capture.CapturedIO at 0x7fdac021c5d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMuLA96xaFFk"
      },
      "source": [
        "model.load_weights('mel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lywUEGAxaIuU"
      },
      "source": [
        "backup =  model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVa2zGk4aIwy"
      },
      "source": [
        "accuracy_score(y_test.argmax(axis=1),backup.argmax(axis=1))\n",
        "# accuracy_score(g,backup.argmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28dDIhWraI2W"
      },
      "source": [
        "encoder = LabelBinarizer()\n",
        "encoder.fit(train['Emotion'])\n",
        "train_label  = encoder.transform(train['Emotion'])\n",
        "test_label  = encoder.transform(test['Emotion'])\n",
        "lab = LabelEncoder().fit(train['Emotion'])\n",
        "f  = lab.transform(train['Emotion'])\n",
        "g  = lab.transform(test['Emotion'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1muOab5amL8"
      },
      "source": [
        "max_length = 128\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "def map_example_to_dict(input_ids, attention_masks, label):\n",
        "    # print(label[0])\n",
        "    return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"attention_mask\": attention_masks,\n",
        "           }, label[0]\n",
        "def encode_examples(ds, limit=-1):\n",
        "  input_id = []\n",
        "  attention_mask = []\n",
        "  labels = []\n",
        "  for one_data, label in tfds.as_numpy(ds):\n",
        "    bert_input = tokenizer.encode_plus(one_data.decode(),add_special_tokens=True,max_length=max_length,pad_to_max_length=True,return_attention_mask=True,)\n",
        "    input_id.append(bert_input['input_ids'])\n",
        "    attention_mask.append(bert_input['attention_mask'])\n",
        "    labels.append([label])\n",
        "\n",
        "  return tf.data.Dataset.from_tensor_slices((input_id,\n",
        "                                             attention_mask,\n",
        "                              labels)).map(map_example_to_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTKnbBeeasLl"
      },
      "source": [
        "\n",
        "training_sentences_modified = tf.data.Dataset.from_tensor_slices((train['Utterance'], train_label)) \n",
        "testing_sentences_modified = tf.data.Dataset.from_tensor_slices((test['Utterance'],test_label)) \n",
        "\n",
        "ds_train_encoded = encode_examples(training_sentences_modified).batch(batch_size) \n",
        "ds_test_encoded = encode_examples(testing_sentences_modified).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7caI5kYeatIu"
      },
      "source": [
        "learning_rate = 1.75e-5\n",
        "number_of_epochs = 5\n",
        "\n",
        "class ModelMetrics(tf.keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.count_n = 1\n",
        "        self.best = 0\n",
        "\n",
        "    def on_epoch_end(self, batch, logs={}):\n",
        "        y_val_pred_train = tf.nn.softmax(self.model.predict(ds_train_encoded)['logits'])\n",
        "        y_pred_argmax_train = tf.math.argmax(y_val_pred_train, axis=1)\n",
        "\n",
        "        y_val_pred = tf.nn.softmax(self.model.predict(ds_test_encoded)['logits'])\n",
        "        y_pred_argmax = tf.math.argmax(y_val_pred, axis=1)\n",
        "        training_copy = {}\n",
        "        training_copy['predicted'] = y_pred_argmax_train\n",
        "        testing_copy = {}\n",
        "        testing_copy['predicted'] = y_pred_argmax \n",
        "\n",
        "        f1_s = accuracy_score(g, testing_copy['predicted'].numpy())\n",
        "        f2_s = accuracy_score(f, training_copy['predicted'].numpy())\n",
        "        micro = f1_score(g, testing_copy['predicted'].numpy(), average='micro')\n",
        "        weight = f1_score(g, testing_copy['predicted'].numpy(), average='weighted')\n",
        "        print('\\n accuracy score is :', f1_s, micro, weight)\n",
        "        if self.best < f1_s:\n",
        "            self.model.save_pretrained('rob.h5')\n",
        "            self.best = f1_s\n",
        "            print(f\"saving model:{f1_s} better than {self.best}\")\n",
        "        else:\n",
        "            print(f\"{self.best} better accuracy not saving\")\n",
        "        self.count_n += 1\n",
        "\n",
        "metrics = ModelMetrics()\n",
        "# model initialization\n",
        "model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels = 7)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-05)\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss) # , metrics=[metric]\n",
        "\n",
        "histCallback = model.fit(ds_train_encoded, epochs=number_of_epochs,\n",
        "          validation_data=ds_test_encoded, callbacks=[metrics]) # metrics is giving issues ,checkpoint\n",
        "                                                           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbyR7WYlazg0"
      },
      "source": [
        "model = TFRobertaForSequenceClassification.from_pretrained(\"rob.h5\", num_labels = 7)\n",
        "# t2 = model.predict( ds_test_encoded  )\n",
        "t2 = model.predict( ds_test_encoded  )\n",
        "textual = tf.nn.softmax(model.predict(ds_test_encoded)['logits']).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeykSMQeGz9m"
      },
      "source": [
        "accuracy_score(g, textual.argmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqPsYEk-hEp3"
      },
      "source": [
        "textual = tf.nn.softmax(model.predict(ds_test_encoded)['logits']).numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4TIH7NyND8E"
      },
      "source": [
        "best =0.5*backup+textual\n",
        "accuracy_score(g, best.argmax(axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc-3DtVmQK6e"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(g, best.argmax(axis=1), average='weighted')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}